{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 선언\n",
    "from confluent_kafka import Producer\n",
    "from pyarrow import fs\n",
    "import pandas as pd\n",
    "import json, os, subprocess\n",
    "\n",
    "# 필요한 전역 변수 선언\n",
    "city_code = [11,26,27,28,29,30,31,36,41,43,44,46,47,48,50,51,52] # zcode에 사용할 시군구 코드\n",
    "url = 'https://apis.data.go.kr/B552584/EvCharger/getChargerStatus?serviceKey=34lIyWAciAsJlGtIZ4ltpLy2sLZDR%2BBRWvAv8RgoADNEd%2BKCgHe84XiSRUwL8JMMIubzsFW3ddjcNlhZHhvJIQ%3D%3D&pageNo={}&numOfRows={}&period=5&zcode={}&dataType=JSON'\n",
    "\n",
    "# 개인화 전역 변수 @@@@@@@@@@@@@@@@@@ 수정 필요 @@@@@@@@@@@@@@@@@@\n",
    "# hadoop 디렉터리\n",
    "hdfs_dir = \"encore_20240614_khy\"\n",
    "# kafka 토픽\n",
    "kafka_topic = \"encore_20240614_khy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in city_code:\n",
    "  com = ['curl', url.format(1, 5, cc)]\n",
    "  rt = subprocess.run(com, capture_output=True, text=True)\n",
    "\n",
    "  total_cnt = json.loads(rt.stdout)['totalCount']\n",
    "  max_page = total_cnt // 50 + 1\n",
    "\n",
    "  data_per_cc = []\n",
    "  for i in range(1, max_page + 1):\n",
    "    com = ['curl', url.format(i, 50, cc)]\n",
    "    rt = subprocess.run(com, capture_output=True, text=True)\n",
    "\n",
    "    temp_item = json.loads(rt.stdout)['items']['item']\n",
    "    data_per_cc.extend(temp_item)\n",
    "\n",
    "  # make dir data if not exist\n",
    "  if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "  \n",
    "  # to csv file\n",
    "  with open(f'data/{cc}.csv', 'w') as f:\n",
    "    # write header\n",
    "    f.write(','.join(data_per_cc[0].keys()) + '\\n')\n",
    "    # write data\n",
    "    for item in data_per_cc:\n",
    "      f.write(','.join(item.values()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs에 접근하기 위한 환경변수 설정\n",
    "classpath = subprocess.Popen([\"/home/hadoop/hadoop/bin/hdfs\", \"classpath\", \"--glob\"],stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ['CLASSPATH'] = classpath.decode('utf-8')\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = \"/home/hadoop/hadoop/lib/native\"\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.160', port=8020, user='hadoop')\n",
    "\n",
    "\n",
    "# 로컬 data 디렉터리에 있는 csv 파일을 hdfs로 전송\n",
    "for cc in city_code:\n",
    "  df = pd.read_csv(f'data/{cc}.csv')\n",
    "  df.fillna('-', inplace=True)\n",
    "  # write to hdfs\n",
    "  with hdfs.open_output_stream(f'/{hdfs_dir}/{cc}.csv') as f:\n",
    "    df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hdfs2Kafka(object):\n",
    "  def __init__(self):\n",
    "    classpath = subprocess.Popen(['/home/hadoop/hadoop/bin/hdfs', 'classpath', '--glob'], stdout=subprocess.PIPE).communicate()[0]\n",
    "    os.environ['CLASSPATH'] = classpath.decode('utf-8')\n",
    "    os.environ['ARROW_LIBHDFS_DIR'] = '/home/hadoop/hadoop/lib/native'\n",
    "    self._hdfs = fs.HadoopFileSystem(host='192.168.0.160', port=8020, user='hadoop')\n",
    "\n",
    "    kafka_brokers = \"192.168.0.201:9092\"\n",
    "    kafka_resetType = \"earliest\"\n",
    "\n",
    "    conf = {'bootstrap.servers': kafka_brokers}\n",
    "    self._producer = Producer(**conf)\n",
    "\n",
    "  def getHdFileInfo(self, filename):\n",
    "    f_Info = self._hdfs.get_file_info(filename)\n",
    "    print(f\"type: {str(f_Info.type)}\")\n",
    "    print(f\"path: {f_Info.path}\")\n",
    "    print(f\"size: {f_Info.size}\")\n",
    "    print(f\"last_modified: {f_Info.mtime}\")\n",
    "    \n",
    "    # return f_Info\n",
    "\n",
    "  def readHdFile(self, filename):\n",
    "    with self._hdfs.open_input_file(filename) as f:\n",
    "      read_data = f.read().decode('utf-8').splitlines()\n",
    "      return [line.split(',') for line in read_data]\n",
    "    \n",
    "  def sendData2Kafka(self, topic, list_line):\n",
    "    for data in list_line:\n",
    "      str_tmp = \",\".join(data).split(\",\")\n",
    "      modified_data = \",\".join(str_tmp[:2]) + \",\" + \",\".join(str_tmp[4:])\n",
    "      print(modified_data)\n",
    "      self._producer.poll(0)\n",
    "      self._producer.produce(topic, modified_data.encode('utf-8'), callback=kafka_producer_call)\n",
    "      self._producer.flush()\n",
    "  \n",
    "  def checkKafkaTopic(self, topic):\n",
    "    topic_list = self._producer.list_topics().topics\n",
    "    if topic in topic_list:\n",
    "      print(f\"topic {topic} is exist\")\n",
    "      return True\n",
    "    else:\n",
    "      print(f\"topic {topic} is not exist\")\n",
    "      return False\n",
    "    \n",
    "  def createKafkaTopic(self, topic):\n",
    "    self._producer.create_topics([topic])\n",
    "\n",
    "def kafka_producer_call(err, msg):\n",
    "  if err is not None:\n",
    "    print(f\"Failed to deliver message, err={str(err)} msg={str(msg)}\")\n",
    "  else:\n",
    "    print(f\"Message produced, topic={msg.topic()} partition=[{msg.partition()}] @ offset={msg.offset()}\")\n",
    "    print(f\"Message.topic: {msg.topic()}\")\n",
    "    print(f\"Message.timestamp: {msg.timestamp()}\")\n",
    "    print(f\"Message.key: {msg.key()}\")\n",
    "    print(f\"Message.value: {msg.value().decode('utf-8')}\")\n",
    "    print(f\"Message.partition: {msg.partition()}\")\n",
    "    print(f\"Message.offset: {msg.offset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic encore_20240614_khy is exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kafka producer 객체 생성\n",
    "kafka_prod = Hdfs2Kafka()\n",
    "\n",
    "# kafka 토픽이 존재하는지 확인\n",
    "if(kafka_prod.checkKafkaTopic(kafka_topic)):\n",
    "  print(\"kafka topic is exist, good to go\")\n",
    "else:\n",
    "  print(\"kafka topic is not exist, create topic\")\n",
    "  kafka_prod.createKafkaTopic(kafka_topic)\n",
    "  print(f\"kafka topic \\\"{kafka_topic}\\\" created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs로부터 csv 파일을 읽어서 kafka로 전송\n",
    "for cc in city_code:\n",
    "  temp = kafka_prod.readHdFile(f'/{hdfs_dir}/{cc}.csv')\n",
    "  kafka_prod.sendData2Kafka(kafka_topic, temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
