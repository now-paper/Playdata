{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4aa745-0516-4542-a2c6-d3b442c68977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://producer:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d226e9f-1c57-4751-a0f9-e0ece2891013",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a105c451-a70f-4dfa-8773-21be29c6a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 실행되지 않고, 객체로 생성되기만 한 상태임 \n",
    "df = spark.\\\n",
    "    read.\\\n",
    "    csv(\"hdfs://192.168.0.160:8020/encore/20240614/civilian_force.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56fed10d-37ec-41a2-a739-53706862a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [date#284,values#285,realtime_start#286,realtime_end#287,state#288,id#289,title#290,frequency_short#291,units_short#292,seasonal_adjustment_short#293] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://192.168.0.160:8020/encore/20240614/civilian_force.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:string,values:string,realtime_start:string,realtime_end:string,state:string,id:string...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 계획을 보는 함수\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9542e2fa-2b0e-4048-8e3a-ec188d230553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+------------+------+--------------------+--------------------+---------------+-----------+-------------------------+\n",
      "|      date|  values|realtime_start|realtime_end| state|                  id|               title|frequency_short|units_short|seasonal_adjustment_short|\n",
      "+----------+--------+--------------+------------+------+--------------------+--------------------+---------------+-----------+-------------------------+\n",
      "|1976-01-01|164014.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1977-01-01|173460.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1978-01-01|182114.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1979-01-01|184638.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1980-01-01|188828.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1981-01-01|198021.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1982-01-01|213405.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1983-01-01|230474.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1984-01-01|243254.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1985-01-01|249601.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1986-01-01|254814.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1987-01-01|249574.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1988-01-01|250991.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1989-01-01|257917.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1990-01-01|270484.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1991-01-01|276211.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1992-01-01|286421.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1993-01-01|294093.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1994-01-01|300786.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "|1995-01-01|302756.0|    2023-01-21|  2023-01-21|Alaska|LAUST020000000000...|Civilian Labor Fo...|              A|    Persons|                      NSA|\n",
      "+----------+--------+--------------+------------+------+--------------------+--------------------+---------------+-----------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스파크에서 사용하는 데이터프레임\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "028c8b50-523c-4eba-bbe6-4cbbcd66e0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('date', StringType(), True), StructField('values', StringType(), True), StructField('realtime_start', StringType(), True), StructField('realtime_end', StringType(), True), StructField('state', StringType(), True), StructField('id', StringType(), True), StructField('title', StringType(), True), StructField('frequency_short', StringType(), True), StructField('units_short', StringType(), True), StructField('seasonal_adjustment_short', StringType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas의 info() 에 해당하는 함수로, 각 컬럼이 어떤 데이터 타입인지 보여줌 \n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "136621e7-0044-416d-bf31-59e3a21614dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferSchema 옵션으로 내가 일일이 데이터 형식 지정하지 않아도 추정해서 schema 만들도록 설정\n",
    "df2 = spark.\\\n",
    "    read.\\\n",
    "    csv(\"hdfs://192.168.0.160:8020/encore/20240614/civilian_force.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a046c595-4759-43e4-bd3e-bf5e7c35b861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('date', DateType(), True), StructField('values', DoubleType(), True), StructField('realtime_start', DateType(), True), StructField('realtime_end', DateType(), True), StructField('state', StringType(), True), StructField('id', StringType(), True), StructField('title', StringType(), True), StructField('frequency_short', StringType(), True), StructField('units_short', StringType(), True), StructField('seasonal_adjustment_short', StringType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df와 달리 DoubleType, DateType 등으로 추론해준 부분이 있다. \n",
    "df2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aceeab8-61a0-478a-81bd-e66d24503b6e",
   "metadata": {},
   "source": [
    "# RDD 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99580061-f043-4516-962e-bfc2fe19faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f86883dd-e6e0-491b-950e-2cedc30fb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"file:///home/kafka/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa5e5e1f-11da-4d79-8ecd-8480104e7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1_2 = rdd1.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a41a965-6450-450d-a053-916bd5374318",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1_3 = rdd1_2.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "179e2507-97b0-4fae-9558-e9557dd8e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1_4 = rdd1_3.reduceByKey(lambda a, b : a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7a64823-3ea6-4e6a-925e-cda69b56082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd1_5 = rdd1_4.map(lambda x: (x[1], x[0])).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "659819dc-299f-41a3-aa0a-fa4b06bdc268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 'Project'), (9, 'Gutenberg’s'), (18, 'Alice’s'), (18, 'in'), (18, 'Lewis'), (18, 'Carroll'), (18, 'Adventures'), (18, 'Wonderland'), (18, 'by'), (27, 'is'), (27, 'use'), (27, 'of'), (27, 'anyone'), (27, 'anywhere'), (27, 'at'), (27, 'no'), (27, 'This'), (27, 'eBook'), (27, 'for'), (27, 'the'), (27, 'cost'), (27, 'and'), (27, 'with')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd1_5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31e82f7c-2c56-4e1b-9b81-7656328d6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 'Project'), (9, 'Gutenberg’s'), (18, 'Alice’s'), (18, 'in'), (18, 'Lewis'), (18, 'Carroll'), (18, 'Adventures'), (18, 'Wonderland'), (18, 'by'), (27, 'is'), (27, 'use'), (27, 'of'), (27, 'anyone'), (27, 'anywhere'), (27, 'at'), (27, 'no'), (27, 'This'), (27, 'eBook'), (27, 'for'), (27, 'the'), (27, 'cost'), (27, 'and'), (27, 'with')]\n"
     ]
    }
   ],
   "source": [
    "# 위 과정을 변수 없이 보통 아래와 같이 씀 \n",
    "\n",
    "print(spark.sparkContext.textFile(\"file:///home/kafka/test.txt\").\\\n",
    "        flatMap(lambda x: x.split(\" \")).\\\n",
    "        map(lambda x: (x, 1)).\\\n",
    "        reduceByKey(lambda a, b : a+b).\\\n",
    "        map(lambda x: (x[1], x[0])).sortByKey().\\\n",
    "        collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055eeb3-e2b4-4b87-bf1c-ddf2d56c528d",
   "metadata": {},
   "source": [
    "# 예제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0b9d654-c0f0-40a4-99bc-ed0b7efb3141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n",
      "|Seqno|Quote                                                                        |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "|1    |Be the change that you wish to see in the world                              |\n",
      "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
      "|3    |The purpose of our lives is to be happy.                                     |\n",
      "|4    |Be cool.                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Quote\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
    "    (\"4\", \"Be cool.\")]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show(truncate=False) # 잘림 없이 끝까지 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdd9ebdb-881a-496d-bc31-3a4db2c941a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a58f41c4-2c26-4575-a8af-71a2bd1e55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7388da4f-3f82-4575-98a5-8d9a7509e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "febd51df-6e81-4402-accb-72195c5a2374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼명 만들어서 넣어주기 \n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226163d-fc25-4538-a390-75b05dc57397",
   "metadata": {},
   "source": [
    "# 예제 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e63972-04a0-494c-902b-973b7e55a658",
   "metadata": {},
   "source": [
    "# 사용하지 않을 건데 전체 패키지 불러오면 예기치 못한 오류 발생할 수 있음. 필요한 것만 불러와서 쓰기! \n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4d96e7f-37f8-434a-ba45-0af14ee00122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "deptSchema = StructType([\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a5c2e-f30f-4b3c-9616-6981c7c55002",
   "metadata": {},
   "source": [
    "# 예제 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e6eedf2-9a10-4fd4-a820-1b7fc3b3d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "# Column names\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0011a0d-dad6-4f77-8d03-5661450765d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(firstname='James', lastname='Smith', country='USA', state='CA')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923cd15-5d7e-404d-9138-0e137acf765a",
   "metadata": {},
   "source": [
    "## 컬럼 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9225e103-e355-4333-8ad5-bc99b05fa12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(firstname='James')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"firstname\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19191e9c-ec20-41e1-8134-438cdff1dc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\", \"lastname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f58d827-1aa6-41b1-9584-be4b23231f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"firstname\"], df[\"lastname\"]).show(3)\n",
    "df.select(df.firstname, df.lastname).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "476612a7-a9cc-413e-9b7b-99edae9d293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+\n",
      "|firstname|lastname|country|\n",
      "+---------+--------+-------+\n",
      "|    James|   Smith|    USA|\n",
      "|  Michael|    Rose|    USA|\n",
      "|   Robert|Williams|    USA|\n",
      "|    Maria|   Jones|    USA|\n",
      "+---------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[:3]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9c165-4dce-4e49-852c-45bbcb85e8ad",
   "metadata": {},
   "source": [
    "# 예제 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "951a8e82-4141-4df5-a005-da3c606780d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType        \n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2476016b-b91f-474a-a765-228e00125508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                name|state|gender|\n",
      "+--------------------+-----+------+\n",
      "|{James, null, Smith}|   OH|     M|\n",
      "|      {Anna, Rose, }|   NY|     F|\n",
      "| {Julia, , Williams}|   OH|     F|\n",
      "|{Maria, Anne, Jones}|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|   NY|     M|\n",
      "|{Mike, Mary, Will...|   OH|     M|\n",
      "+--------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1f2d0cf-e30f-4b02-80e8-f2eb245bd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name=Row(firstname='James', middlename=None, lastname='Smith'), state='OH', gender='M')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d79382f6-72f5-40f6-a619-eb6f1a9196e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|name                |\n",
      "+--------------------+\n",
      "|{James, null, Smith}|\n",
      "|{Anna, Rose, }      |\n",
      "|{Julia, , Williams} |\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba8f378f-ff77-48b1-bbf9-348df59783cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|firstname|lastname|gender|\n",
      "+---------+--------+------+\n",
      "|James    |Smith   |M     |\n",
      "|Anna     |        |F     |\n",
      "|Julia    |Williams|F     |\n",
      "+---------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.firstname\", \"name.lastname\", \"gender\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a72bc309-b71c-4e0e-b9a7-87236e0aa592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+------+\n",
      "|firstname|middlename|lastname|gender|\n",
      "+---------+----------+--------+------+\n",
      "|    James|      null|   Smith|     M|\n",
      "|     Anna|      Rose|        |     F|\n",
      "|    Julia|          |Williams|     F|\n",
      "+---------+----------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.*\", \"gender\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036c959-cce8-4275-aa5d-6d44bab96e59",
   "metadata": {},
   "source": [
    "# 예제 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d773cafb-8d03-458a-8618-887660aa1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c67e353-af04-456a-90c6-77145eaf3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1f82c0c-c8ac-4d95-8de4-738452ea4e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.sum(col: 'ColumnOrName') -> pyspark.sql.column.Column>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리가 작업하는 네임스페이스 안에 겹치는 이름이 있으면 둘 중 하나는 포기해야 함 (python sum VS sql sum)\n",
    "# del sum 하면 pyspark의 sum이 네임스페이스에서 삭제되고 python의 sum으로 다시 돌아온다 \n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26ae565c-6a2d-4a4d-9fb3-80a3b0ddfe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby(\"department\").\\\n",
    "        agg(sum('salary').alias('sum_salary'),\\\n",
    "        avg('salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f71da750-b126-463e-aa00-24343ded3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[department#626], functions=[sum(salary#628L), avg(salary#628L)])\n",
      "   +- Exchange hashpartitioning(department#626, 200), ENSURE_REQUIREMENTS, [plan_id=346]\n",
      "      +- HashAggregate(keys=[department#626], functions=[partial_sum(salary#628L), partial_avg(salary#628L)])\n",
      "         +- Project [department#626, salary#628L]\n",
      "            +- Scan ExistingRDD[employee_name#625,department#626,state#627,salary#628L,age#629L,bonus#630L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tmp 라는 df를 만들 계획\n",
    "tmp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "151f8e22-696e-4aec-84b1-a1b9ec33fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+\n",
      "|department|sum_salary|      avg(salary)|\n",
      "+----------+----------+-----------------+\n",
      "|     Sales|    257000|85666.66666666667|\n",
      "|   Finance|    351000|          87750.0|\n",
      "| Marketing|    171000|          85500.0|\n",
      "+----------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실제로 액션 수행\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61459896-5d56-4ba2-8565-97589d69937e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4eb653-6ce6-4aa1-9c5e-7ab38ca88b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
